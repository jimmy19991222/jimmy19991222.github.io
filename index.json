[{"content":"Chapter 2 Probability Distributions Chapter Structure Exponential family distribution：\nExponential family distribution has a fixed formed conjugate prior.\nBernoulli distribution\nConjugate Prior: Beta distribution\nOne binary experiment: toss a coin\nBinomial distribution\nConjugate Prior: Beta distribution\nN binary experiments: toss a coin many times\nMultinomial distribution\nConjugate Prior: Dirichlet distribution\nMultiple value experiments: like throw the dice\nGaussian Distribution\nConjugate Prior of Mean: Gaussain distribution\nConjugate Prior of Variance: Inverse Gamma distribution\n1 dimension/ n dimensions\nmarginal distribution/ conditional distribution\nPosteriori distribution\nConvolution\nParameter estimation:\nMaximum Likelihood Estimation Sequence estimation Approximate evaluation Non-informative prior distribution:\nStudent t-distribution Periodic variable GMM (Not a kind of Exponential family distribution) Density Estimation $\\text{i.i.d. samples: }x ={x_1, x_2, \u0026hellip;, x_n},\\ \\text{probability density: } p(x)=\\left{ \\begin{aligned} \u0026amp; \\text{parameterized}\\ \u0026amp; \\text{non-parameterized} \\end{aligned} \\right.$\ni.i.d.: independent and identically distribution\nParameter models: parameters control all the information of the model, such as mean and variance of Gaussaian distribution. Need time for training to estimate parameters. No need for training samples when inference.\nNon-parameter models: parameters only control complexity of the model, such as k of k-means model.\nBernoulli distribution $x\\in {0, 1},\\ p(x=0|\\mu)=1-\\mu\\ \\text{Bern}(x|\\mu) = \\mu^x(1-\\mu)^{1-x}$\nPerform once binary variable experiment, we get Bernoulli distribution / binary distribution.\nmean and vairance given by,\n$E[x]=\\sum_{x\\in{0,1}}xp(x)=0p(x=0)+1p(x=1)=\\mu\\ var[x] = \\sum(x-E[x])^2p(x)\\=E[x]^2p(x=0)+(1-E[x])^2p(x=1)=\\mu(1-\\mu)$\nGiven a data set $D(x_1, \u0026hellip;, x_N)$ , estimate the parameter $\\mu$, on the assumption taht the observations are drawn independently from $p(x|\\mu)$, so that\n$p(D|\\mu) = \\prod_{n=1}^Np(x_n|\\mu) = \\prod_{n=1}^N \\mu^{x_n}(1-\\mu)^{1-x_n}\\ \\ln p(D|\\mu) = \\sum_{n=1}^Np(x_n|\\mu) = \\sum_{n=1}^N{x_n\\ln \\mu + (1-x_n)\\ln (1-\\mu)}$\nLet $\\frac{\\part \\ln p(D|\\mu)}{\\part \\mu}=0$,\n$\\sum_{n=1}^N{\\frac{x_n}\\mu - \\frac{(1-x_n)}{(1-\\mu)}} = 0$\nThen,\n$\\sum_{n=1}^N\\frac{x_n}\\mu =\\sum_{n=1}^N\\frac{(1-x_n)}{(1-\\mu)}\\ (1-x_n)\\sum_{n=1}^N x_n = x_n\\sum_{n=1}^N(1-x_n)\\ \\mu = \\frac{1}{N}\\sum_{n=1}^N x_n$\nDefine function $S = T(x_1, \u0026hellip;,x_n)$, if there are no unknown parameters, $S$ is a statistic for the data under distribution.\nDefine the density function depends on parameter $\\theta$, if $p(x|\\theta)$ can be written in the form\n$p(x|\\theta) = h(x)g_{\\theta}(\\Phi(x))$\n$h(x)$ is independent with $\\theta$, then $\\Phi(x)$ or $\\theta$ is a Sufficient statistic for the data under distribution.\nWe can say the sum of samples or the mean of samples is a Sufficient statistic for the data under Bernoulli distribution.\nBinomial distribution $\\text{Bin}(m|N, \\mu) = \\left( \\begin{aligned} m\\n \\end{aligned} \\right) \\mu^m(1-\\mu)^{N-m}$\nwhere\n$\\left( \\begin{aligned} m\\n \\end{aligned} \\right) = \\frac{N!}{(N-m)!m!}$\nthe mean and variance are given by\n$E[m] = \\sum_{m=0}^N m\\text{Bin}(m|N, \\mu) = N\\mu\\ var[m] = \\sum_{m=0}^N(m-E[m])^2\\text{Bin}(m|N, \\mu) = N\\mu(1-\\mu)$\nBeta distribution ","permalink":"https://jimmy19991222.github.io/posts/prml/probability_distributions/","summary":"Chapter 2 Probability Distributions Chapter Structure Exponential family distribution：\nExponential family distribution has a fixed formed conjugate prior.\nBernoulli distribution\nConjugate Prior: Beta distribution\nOne binary experiment: toss a coin\nBinomial distribution\nConjugate Prior: Beta distribution\nN binary experiments: toss a coin many times\nMultinomial distribution\nConjugate Prior: Dirichlet distribution\nMultiple value experiments: like throw the dice\nGaussian Distribution\nConjugate Prior of Mean: Gaussain distribution\nConjugate Prior of Variance: Inverse Gamma distribution","title":"PRML | Probability Distributions"},{"content":"bounding box ground truth\n4 number to decide a bounding box $$ (x_{left-up}, y_{left-up},x_{right-down}, y_{right-down})\\ (x_{left-up}, y_{left-up},width, height) $$ for drawing, another kind $$ (x_{center}, y_{center}, width, height) $$ Dataset\nevery row represent a object File_name, object_class, bounding box(4 value) COCO dataset 80 object class, 330K picture, 1.5M objects anchor box A lot of anchor-box-based algorithm.\na series of predictive box. predict whether every anchor box has interested object if yes, predict the offset between anchor box and bounding box Every anchor box is a training sample.\nanchor box is either negative sample or related to a bounding box\nWe will generate huge amount of anchor box,\nthis will lead to a lot of negative anchor box a matrix to record IOU. two dimension (anchor box index, bounding box index). Assign the largest IOU (global) to matched bounding box. IOU Intersection over Union (IOU) is to compute the similarity between two boxes.\n0 is no overlap, 1 is full overlap A special case of Jacquard index\ntwo given set A and B $$ J(A, B) = \\frac{|A\\cap B|}{|A\\cup B|} $$ NMS Non-Maximum Suppression\nEvery anchor box predict a bounding box NMS can merge those similar anchor boxes select the largest predict value(confident) and class is not negative(background) Abondon other boxes which IOU larger than a threshold repeat these steps How to generate anchor boxes? generate those anchor boxes with widths and heights as $ws\\sqrt{r}$ and $hs/\\sqrt{r}$, where w, h is width and height of the picture, s is scale of anchor box, r is the ratio. It is over all the pixel in picture.\nCommon method is to consider best scale $s_1$ and with all the $r$, and $r=1$ with all $s$ $$ (s_1, r_1), (s_1, r_2),\u0026hellip;,(s_1, r_m),(s_2, r_1), (s_2, r_1),\u0026hellip;,(s_n, r_1) $$ number is n+m-1\nR-CNN Region-CNN (R-CNN)\nSelective search to select anchor box pre-trained model to get features from every anchor box train a SVM to classify train a Linear regression model to predict the offset fo every box ROI pooling to get different size anchor boxes batched\nFast RCNN get feature from CNN find feature map by same-proportion dividing of anchor box RoI pooling Full-connected layer Faster RCNN Two stage. Use region proposal network(RPN) to substitute the selective search\nRPN input: global feature map\noutput: high-quality anchor box after NMS\nRPN: use CNN to binary classify anchor boxes and regression the offset\nmap is high, but very slow\nMask R-CNN After RoI align, use fully convolutional network(FCN) to get mask prediction\ndo semantic segmentation\nSSD single shot detection\nMulti-scale feature\nwithout RPN, all anchor boxes are predicted\nLower stage to detect the small object, upper stage to detect the large object\nssd is fast, but map is very low.\nYOLO you only look once\nSSD have those anchor boxes overlapped. A large waste of computation.\nuniformly separate picture into SxS anchor boxes. Predict anchor box B bounding box. because anchor box may include multiple objects. faster than SSD ","permalink":"https://jimmy19991222.github.io/posts/object_detection/object-detection/","summary":"bounding box ground truth\n4 number to decide a bounding box $$ (x_{left-up}, y_{left-up},x_{right-down}, y_{right-down})\\ (x_{left-up}, y_{left-up},width, height) $$ for drawing, another kind $$ (x_{center}, y_{center}, width, height) $$ Dataset\nevery row represent a object File_name, object_class, bounding box(4 value) COCO dataset 80 object class, 330K picture, 1.5M objects anchor box A lot of anchor-box-based algorithm.\na series of predictive box. predict whether every anchor box has interested object if yes, predict the offset between anchor box and bounding box Every anchor box is a training sample.","title":"Object detection | Overview"},{"content":"$\\text{Q}_1$: Who are you?\n$A$: Hi, this is Jieming. I am currently a NUS DSML master, and a research engineer intern at A*STAR as well. My research interest focused on Adversarial Robustness and Multimodal Learning. ","permalink":"https://jimmy19991222.github.io/faq/","summary":"$\\text{Q}_1$: Who are you?\n$A$: Hi, this is Jieming. I am currently a NUS DSML master, and a research engineer intern at A*STAR as well. My research interest focused on Adversarial Robustness and Multimodal Learning. ","title":"FAQ"}]