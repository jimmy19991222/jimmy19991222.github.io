[{"content":"Background Similarity measurement(PSNR and SSIM) PSNR Peak signal-to-noise ratio (PSNR) is to check the similarity of differences.\nThe definition starts from the mean squared error. $$ MSE = \\frac{1}{c\\cdot h\\cdot w}\\sum(I_1-I_2)^2 $$ where $I_1$ and $I_2$ are two images, $h$ and $w$ are the height and width of images, $c$ is the numbers of channels.\nThen PSNR is expressed as $$ PSNR = 10\\cdot \\log_{10} \\left(\\frac{MAX_I^2}{MSE}\\right) $$ where the $MAX_I$ is the maximum valid value for a pixel.\nIn case of MSE is 0, we need to handle this separately.\nThe logarithmic scale is made because the pixel values have a very wide dynamic range.\nSSIM Humans are not sensitive to the absolute brightness/color of pixels, but are very sensitive to the position of edges and textures. Structural Similarity Index(SSIM) mimics human perception by focusing primarily on edge and texture similarity.\nWe cut pictures into patches and compare them patch by patch to compute SSIM. Given patch $x$ from image $I_1$ and $y$ from another image $I_2$, we compute several statistics, including\n$\\mu_x$, $\\mu_y$: the average of $x$ / $y$ $\\sigma_x^2$, $\\sigma_y^2$: the variance of $x$ / $y$ $\\sigma_{x,y}$: the covariance of $x$ and $y$ Then the similarity of luminance between patches can be expressed as $$ l(x, y)=\\frac{2\\mu_x\\mu_y}{\\mu_x^2+\\mu_y^2} $$ If there is a big difference between $x$ and $y$, $l(x,y)$ will be close to 0; If they have similar luminance, $l(x,y)$ will be close to 1. $l(x, y)$ is scale-invariance.\nThe similarity of contrast between patches can be expressed as $$ c(x,y) = \\frac{2\\sigma_x\\sigma_y}{\\sigma_x^2+\\sigma_y^2} $$ If one patch is much \u0026ldquo;flat\u0026rdquo; than the other, the score close to 0; If both have the same contrast level, the score is close to 1. Contrast score compares the number of \u0026ldquo;textures\u0026rdquo; in an image block. This formula is also scale-invariant.\nThe similarity of structure between patches can be expressed as $$ s(x, y) = \\frac{\\sigma_{x,y}}{\\sigma_x\\sigma_y} $$ Scores are high when two patches contain edges with the same position and orientation; while low if the patches differ on the location of the edges.\nThe overall SSIM score is the product of these three scores. Some small constants are added to prevent division by zero. SSIM will return a similarity index averaged over all channels of the image. The value is between 0 and 1, where 1 corresponds to perfect fit.\nPapers PSENet: Progressive Self-Enhancement Network for Unsupervised Extreme-Light Image Enhancement Most work focused on over-expose condition, while PSENet focused on under-expose condition.\nIn-camera image signal processors usually use highly nonlinear operations to generate the final 8-bit standard RGB image.\nReference Similarity measurement (PSNR and SSIM) All about Structural Similarity Index (SSIM) 什么是SSIM？ PSENet: Progressive Self-Enhancement Network for Unsupervised Extreme-Light Image Enhancement ","permalink":"https://jimmy19991222.github.io/posts/enhencement/lowlight_enhance/","summary":"Background Similarity measurement(PSNR and SSIM) PSNR Peak signal-to-noise ratio (PSNR) is to check the similarity of differences. The definition starts from the mean squared error. $$ MSE = \\frac{1}{c\\cdot h\\cdot w}\\sum(I_1-I_2)^2 $$ where $I_1$ and $I_2$ are two images, $h$ and $w$ are the height and width of images, $c$ is the numbers of channels. Then PSNR is expressed as $$ PSNR = 10\\cdot \\log_{10} \\left(\\frac{MAX_I^2}{MSE}\\right) $$ where the $MAX_I$","title":"Enhancement | Exposure Correction"},{"content":"bounding box ground truth\n4 number to decide a bounding box $$ (x_{left-up}, y_{left-up},x_{right-down}, y_{right-down})\\ (x_{left-up}, y_{left-up},width, height) $$ for drawing, another kind $$ (x_{center}, y_{center}, width, height) $$ Dataset\nevery row represent a object File_name, object_class, bounding box(4 value) COCO dataset 80 object class, 330K picture, 1.5M objects anchor box A lot of anchor-box-based algorithm.\na series of predictive box. predict whether every anchor box has interested object if yes, predict the offset between anchor box and bounding box Every anchor box is a training sample.\nanchor box is either negative sample or related to a bounding box\nWe will generate huge amount of anchor box,\nthis will lead to a lot of negative anchor box a matrix to record IOU. two dimension (anchor box index, bounding box index). Assign the largest IOU (global) to matched bounding box. IOU Intersection over Union (IOU) is to compute the similarity between two boxes.\n0 is no overlap, 1 is full overlap A special case of Jacquard index\ntwo given set A and B $$ J(A, B) = \\frac{|A\\cap B|}{|A\\cup B|} $$ NMS Non-Maximum Suppression\nEvery anchor box predict a bounding box NMS can merge those similar anchor boxes select the largest predict value(confident) and class is not negative(background) Abondon other boxes which IOU larger than a threshold repeat these steps How to generate anchor boxes? generate those anchor boxes with widths and heights as $ws\\sqrt{r}$ and $hs/\\sqrt{r}$, where w, h is width and height of the picture, s is scale of anchor box, r is the ratio. It is over all the pixel in picture.\nCommon method is to consider best scale $s_1$ and with all the $r$, and $r=1$ with all $s$ $$ (s_1, r_1), (s_1, r_2),\u0026hellip;,(s_1, r_m),(s_2, r_1), (s_2, r_1),\u0026hellip;,(s_n, r_1) $$ number is n+m-1\nR-CNN Region-CNN (R-CNN)\nSelective search to select anchor box pre-trained model to get features from every anchor box train a SVM to classify train a Linear regression model to predict the offset fo every box ROI pooling to get different size anchor boxes batched\nFast RCNN get feature from CNN find feature map by same-proportion dividing of anchor box RoI pooling Full-connected layer Faster RCNN Two stage. Use region proposal network(RPN) to substitute the selective search\nRPN input: global feature map\noutput: high-quality anchor box after NMS\nRPN: use CNN to binary classify anchor boxes and regression the offset\nmap is high, but very slow\nMask R-CNN After RoI align, use fully convolutional network(FCN) to get mask prediction\ndo semantic segmentation\nSSD single shot detection\nMulti-scale feature\nwithout RPN, all anchor boxes are predicted\nLower stage to detect the small object, upper stage to detect the large object\nssd is fast, but map is very low.\nYOLO you only look once\nSSD have those anchor boxes overlapped. A large waste of computation.\nuniformly separate picture into SxS anchor boxes. Predict anchor box B bounding box. because anchor box may include multiple objects. faster than SSD ","permalink":"https://jimmy19991222.github.io/posts/object_detection/object-detection/","summary":"bounding box ground truth\n4 number to decide a bounding box $$ (x_{left-up}, y_{left-up},x_{right-down}, y_{right-down})\\ (x_{left-up}, y_{left-up},width, height) $$ for drawing, another kind $$ (x_{center}, y_{center}, width, height) $$ Dataset\nevery row represent a object File_name, object_class, bounding box(4 value) COCO dataset 80 object class, 330K picture, 1.5M objects anchor box A lot of anchor-box-based algorithm.\na series of predictive box. predict whether every anchor box has interested object if yes, predict the offset between anchor box and bounding box Every anchor box is a training sample.","title":"Object detection | Overview"},{"content":"$\\text{Q}_1$: Who are you?\n$A$: Hi, this is Jieming. I am currently a NUS DSML master, and a research engineer intern at SenseTime as well. My research interest focused on Adversarial Robustness and Multimodal Learning. ","permalink":"https://jimmy19991222.github.io/faq/","summary":"$\\text{Q}_1$: Who are you?\n$A$: Hi, this is Jieming. I am currently a NUS DSML master, and a research engineer intern at SenseTime as well. My research interest focused on Adversarial Robustness and Multimodal Learning. ","title":"FAQ"}]